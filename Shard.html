<!DOCTYPE HTML>
<!--
    Phantom by HTML5 UP
    html5up.net | @ajlkn
    Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
    <title>SERI MATS</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets/css/main.css" />
    <noscript>
        <link rel="stylesheet" href="assets/css/noscript.css" />
    </noscript>
</head>

<body class="is-preload">

    <!----
        <script>
            var url_string = window.location.href; //window.location.href
            var url = new URL(url_string);
            var c = url.searchParams.get("name");
            document.getElementById("pdf").src = c;
        </script>

        <iframe src="" id="pdf" width="100%" height="1200px"></iframe>
        <!---->

    <!-- Wrapper -->
    <div id="wrapper">

        <h1>AI Alignment Research SERI MATS</h1>

        <h3>PPO and ACTDE comparison for mode collapse</h3>
        <p>In this toy experiment, the model plays prisoner's dilemmas against its past self,
            similar to the idea by <a href="https://arxiv.org/abs/2009.09153">Krueger et. al.</a>
            We are not training via self play against a copy. Instead, the model at time <i>t</i> plays against its
            action at time <i>t-1</i>.
            In this environment there are three main strategies, always cooperate, always defect and alternating.
            The reward matrix was selected so that alternating would be the optimal strategy. </p>

        <img src="images/actde-t1-1000.jpg">

        <p>We showed that ACTDE doesn't mode collapse on a toy prisoner's
            dilemma learning task, but instead trains a mixed strategy. For more details see the
            <a
                href="https://www.lesswrong.com/posts/A7RgYuYH4HywNeYWD/mode-collapse-in-rl-may-be-fueled-by-the-update-equation">post
                on lesswrong here.</a>
        </p>

        <a href="https://github.com/MichaelEinhorn/prisonerUnitTest">prisonerUnitTest repo</a> <br><br>

        <h3>Trl-textworld</h3>
        <p>I was in the <a href="www.serimats.org">Stanford Existential Risk Initiative Machine Learning Alignment
                Theory Scholars (SERI MATS) program</a>.
            I was the ML Engineer on Team Shard under the Agent Foundations Stream mentored by John Wentworth.</p>

        <p><b>Overall Thoughts</b><br>
            If we were to resume this project, I think we would start with reevaluating the direction.
            It may be the case that GPT-Neox 20B and Textworld were not the right model and environment to study how
            language models learn with RL finetuning.
            While the model didn't end up being competent enough at the game to investigate how it learned to play,
            I think it was still a valuable learning experience for how to set up experiments with language models, and
            how PPO works. </p>

        <a href="https://github.com/MichaelEinhorn/trl-textworld">trl-textworld repo</a> <br>


        <p>We are fine tuning a language model to play text adventures with RL and Rejection Sampling and will
            interpret what this optimization process changes in the network with interpretability tools, and
            measuring out of distribution behavior and KL divergence.
            We will use these experiments to create a theory of value learning in different optimization procedures.
            <br>
            Some of the considerations that led into choosing Language Models on text adventure games were:
            <br>
            Language Models understand many human concepts from pretraining.
            We believe that a Large Language Model with some form of Fine Tuning is the route we will take to AGI.
            We wanted a single player game that we could vary from simple to complex as we scale the size of the model.
            We wanted an environment for the model to interact with, and not just a single prompt and response.
            <br>
            The current goal is to create multiple competent models trained with these different methods for comparison.
            I am training Gpt-Neox 20 Billion distributed across 8 GPUs with Deepspeed in Pytorch Lightning.
            <br>
            It appears that there is a general consensus that RL is likely more dangerous than i.i.d. training,
            but its not clear which feature of RL is the
            primary cause of this concern and how far it generalizes to similar approaches.
            I expect agency to not be a single concept, but a collection of features that can be learned separately.
            It's not clear which of these features are the main concern for making an agent dangerous,
            or how different training processes might bias policies to learn a different mix of these features.
            I expect the same to be true of RL like finetunings, but it could be the case that many of the methods
            we currently have are just more convoluted ways of getting the same thing.
            Having language models trained with different methods on a simple task with an objective simple reward
            function would be helpful to test for any differences.
            <br>

        <p>
            The model is initially learning, but gets stuck and then become incoherent in the
            middle of the game where there are more options available.
            Prompt engineering and hyperparameter tuning has significantly improved this, though it is not quite
            competent yet.
            A few of the common failures I have seen are the model becoming incoherent and spamming random
            tokens from other languages, collapse into spamming a single token which was most often take,
            or always copy pasting the previous action from the context. In the first failure the completions
            have 0 probability, so it is likely the model learned from random noise.
            The other 2 failures have outputs of high probabilities and look more like the
            <a href="https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse">mode collapse</a>
            seen in RLHF models.
            <br>
            One of the interpretability tools we are interested in using is <a
                href="https://github.com/norabelrose/white-box">tuned lens</a>, which looks for what tokens are
            being considered in each layer of the model. One of the things we would be looking for is where in the
            model does mode collapse occur, and which layers changed the most compared to the reference model.
            <br>
            Some behavioral tests we are interested in are bringing the model back to the pretraining distribution
            to see if it is still competent at general text prediction, and if it has retained values around useful
            items in the game such
            as the key outside of the game's context. We would also test if the model is better able to predict
            the text adventures response.
            We will be looking at multiple checkpoints from different times in the training process, and comparing
            this to different points on the linear path from the pretrained and finetuning network.
        </p>

        <p>Check out this post by David Udell, <a
                href="https://www.lesswrong.com/posts/ZNXDRGshgoq3cmxhB/the-shard-theory-alignment-scheme">The Shard
                Theory Alignment Scheme,</a>
            for more info on our team's ideas.</p>

        <!---->
        <!---->
        <!-- Header -->
        <header id="header">
            <div class="inner">

                <!-- Nav -->
                <nav>
                    <ul>
                        <li><a href="#menu">Menu</a></li>
                    </ul>
                </nav>

            </div>
        </header>


        <!-- Menu -->
        <nav id="menu">
            <h2>Reports</h2>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="Orca.html">Orca IoT Marketing Demo</a></li>
                <li><a href="GTRIScalingStudy.html">ML Scaling Study and Mixup GTRI</a></li>
                <li><a href="MLClassProject.html">Semantic Segmentation ML Class Project</a></li>
                <li><a href="Unisim.html">Simulation of Trading on Uniswap Class Project</a></li>
                <li><a href="Composer.html">Blending Songs using an Autoencoder</a></li>
                <li><a href="Carnegie.html">Carnegie Mellon Game Design in Unity 3D</a></li>
                <li><a href="Shard.html">AI Alignment Research SERI MATS</a></li>
                <li><a href="FedRL.html">Benchmarking Federated RL</a></li>
                <li><a href="resume.html">Resume</a></li>

            </ul>
        </nav>


    </div>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>

</body>

</html>