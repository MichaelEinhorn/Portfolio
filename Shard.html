<!DOCTYPE HTML>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>SERI MATS</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
    <body class="is-preload">

        <!----
        <script>
            var url_string = window.location.href; //window.location.href
            var url = new URL(url_string);
            var c = url.searchParams.get("name");
            document.getElementById("pdf").src = c;
        </script>

        <iframe src="" id="pdf" width="100%" height="1200px"></iframe>
        <!---->

        <!-- Wrapper -->
        <div id="wrapper">

            <h1>AI Alignment Research SERI MATS</h1>
            
            <h3>PPO and ACTDE comparison for mode collapse</h3>
                <p>In this toy experiment, the model plays prisoner's dilemmas against its past self, 
                    similar to the idea by <a href="https://arxiv.org/abs/2009.09153">Krueger et. al.</a> 
                    We are not training via self play against a copy. Instead the model at time <i>t</i> plays against its action at time <i>t-1</i>.
                    In this environment there are three main strategies, always cooperate, always defect and alternating.
                    The reward matrix was selected so that alternating would be the optimal strategy. </p>

                    <img src="images/actde-t1-1000.jpg">

                    <p>We showed that ACTDE doesn't mode collapse on a toy prisoner's 
                    dilemma learning task, but instead trains a mixed strategy. For more details see the 
                    <a href="https://www.lesswrong.com/posts/A7RgYuYH4HywNeYWD/mode-collapse-in-rl-may-be-fueled-by-the-update-equation">post on lesswrong here.</a></p>
                
                 <a href="https://github.com/MichaelEinhorn/prisonerUnitTest">prisonerUnitTest repo</a> <br><br>

            <h3>Trl-textworld</h3>
            <p>I was in the <a href="www.serimats.org">Stanford Existential Risk Initiative Machine Learning Alignment Theory Scholars (SERI MATS) program</a>.
                 I was the ML Engineer on Team Shard under the Agent Foundations Stream mentored by John Wentworth.</p>

                <p><b>Overall Thoughts</b><br>
                If we were to resume this project, I think we would start with reevaluating the direction. 
                It may be the case that GPT-Neox 20B and Textworld were not the right model and environment to study how language models learn with RL finetuning.
                While the model didn't end up being competant enough at the game to investigate how it learned to play, 
                I think it was still a valuable learning experience for how to set up experiments with language models, and how PPO works. </p>

                <a href="https://github.com/MichaelEinhorn/trl-textworld">trl-textworld repo</a> <br>
                
            
            <p>We are fine tunning a language model to play text adventures with RL and Rejection Sampling and will
                interpret what this optimization process changes in the network with interpretability tools,
                measuring out of distribution behavior and KL divergence.
                We will use these experiments to create a theory of value learning in different optimization procedures. 
                <br>
                Some of the considerations that led into choosing Language Models on text adventure games were: 
                <br>
                Language Models understand many human concepts from pretraining. 
                We believe that a Large Language Model with some form of Fine Tuning is the route we will take to AGI.
                We wanted a single player game that we could vary from simple to complex as we scale the size of the model.
                We wanted an environment for the model to interact with, and not just a single prompt and response. 
            <br>
                The current goal is to create multiple competant models trained with these different methods for comparison.
                I am training Gpt Neox 20 Billion distributed across 8 GPUs with Deepspeed in Pytorch Lightning.
                <br>
                 It appears that there is a general consensus that RL is likely more dangerous than iid training, 
                but its not clear which feature of RL is the
                primary cause of this concern and how far it generalizes to similar approaches.
                I expect agency to not be a single concept, but a collection of features that can be learned separately. 
                It's not clear which of these features are the main concern for making an agent dangerous, 
                or how different training processes might bias policies to learn a different mix of these features. 
                I expect the same to be true of RL like finetunings, but it could be the case that many of the methods 
                we currently have are just more convoluted ways of getting the same thing. 
                Having language models trained with different methods on a simple task with an objective simple reward function would be helpful to test for any differences.
                <br>

                <p>
                The model is initially learning, but gets stuck and then become incoherent in the 
                middle of the game where there are more options available.
                Prompt engineering and hyperparameter tuning has significantly improved this, though it is not quite competant yet. 
                A few of the common failures I have seen are the model becoming incoherent and spamming random 
                tokens from other languages, collapse into spamming a single token which was most often take, 
                or always copy pasting the previous action from the context. In the first failure the completions 
                have 0 probability, so it is likely the model learned from random noise. 
                The other 2 failures have outputs of high probabilities and look more like the 
                <a href="https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse">mode collapse</a> seen in RLHF models.
                <br>
                One of the interpretability tools we are interested in using is <a href="https://github.com/norabelrose/white-box">tuned lens</a>, which looks for what tokens are 
                being considered in each layer of the model. One of the things we would be looking for is where in the 
                model does mode collapse occur, and which layers changed the most compared to the reference model.
                <br>
                Some behavioral tests we are interested in are bringing the model back to the pretraining distribution 
                to see if it is still competent at general text prediction, and if it has retained values around useful items in the game such 
                as the key outside of the game's context. We would also test if the model is better able to predict 
                the text adventures response.
                We will be looking at multiple checkpoints from different times in the training process, and comparing 
                this to different points on the linear path from the pretrained and finetuning network.
                </p>

            <p>Check out this post by David Udell, <a href="https://www.lesswrong.com/posts/ZNXDRGshgoq3cmxhB/the-shard-theory-alignment-scheme">The Shard Theory Alignment Scheme,</a>
            for more info on our team's ideas.</p>

            <!---->
            <!---->
            <!-- Header -->
            <header id="header">
                <div class="inner">


                    <!-- Nav -->
                    <nav>
                        <ul>
                            <li><a href="#menu">Menu</a></li>
                        </ul>
                    </nav>

                </div>
            </header>



            <!-- Menu -->
            <nav id="menu">
                <h2>Reports</h2>
                <ul>
                    <li><a href="index.html">Home</a></li>
                    <li><a href="Orca.html">Orca IoT Marketing Demo</a></li>
                    <li><a href="GTRIScalingStudy.html">ML Scaling Study and Mixup GTRI</a></li>
                    <li><a href="MLClassProject.html">Semantic Segmentation ML Class Project</a></li>
                    <li><a href="Unisim.html">Simulation of Trading on Uniswap Class Project</a></li>
                    <li><a href="Composer.html">Blending Songs using an Autoencoder</a></li>
                    <li><a href="Carnegie.html">Carnegie Mellon Game Design in Unity 3D</a></li>
                    <li><a href="Shard.html">AI Alignment Research SERI MATS</a></li>
                    <li><a href="FedRL.html">Benchmarking Federated RL</a></li>
                    <li><a href="resume.html">Resume</a></li>

                </ul>
            </nav>



        </div>

        <!-- Scripts -->
        <script src="assets/js/jquery.min.js"></script>
        <script src="assets/js/browser.min.js"></script>
        <script src="assets/js/breakpoints.min.js"></script>
        <script src="assets/js/util.js"></script>
        <script src="assets/js/main.js"></script>

    </body>
</html>