<!DOCTYPE HTML>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>SERI MATS</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
    <body class="is-preload">

        <!----
        <script>
            var url_string = window.location.href; //window.location.href
            var url = new URL(url_string);
            var c = url.searchParams.get("name");
            document.getElementById("pdf").src = c;
        </script>

        <iframe src="" id="pdf" width="100%" height="1200px"></iframe>
        <!---->

        <!-- Wrapper -->
        <div id="wrapper">

            <h1>AI Alignment Team Shard</h1>
            <p>I am in the Stanford Existential Risk Initiative Machine Learning Alignment Theory Scholars (SERI MATS) program.
                 I am the ML Engineer on Team Shard under the Agent Foundations Stream mentored by John Wentworth.</p>
            <p>It appears that there is a general consensus that RL seems to be more dangerous than iid training, but its not clear which feature of RL is the
                primary cause of this concern and how far it generalizes to similar approaches.
We are fine tunning a language model to play text adventures with RL and Rejection Sampling and will
                interpret what this optimization process changes in the network with interpretability tools,
                measuring out of distribution behavior and KL divergence. We are designing an optimization procedure which gives process level
                feedback on chains of thought in addition to behavior feedback on the task.
                We will use these experiments to create a theory of value learning in different optimization procedures.
</p>
            <p>Some of the considerations that led into choosing Language Models on text adventure games were: <br>
                Language models understand many human concepts from pretraining.
            We believe that a Large Language Model with some form of Fine Tuning is the route we will take to AGI.
            We wanted a single player game that we could vary from simple to complex as we scale the size of the model.
            We wanted an environment for the model to interact with, and not just a single prompt and response. </p>
            <p>My current progress is making a working implementation of PPO and Rejecion Sampling for Language Models.
                I can train a 20 Billion Parameter Transformer distributed across 8 GPUs with Deepspeed in Pytorch Lightning.
                Some of the next steps are to create models trained on text adventure games with RL and Rejection Sampling, and compare the results.
                Run a similar experiment with larger models and chain of thought reasoning on RL and Rejection Sampling.
                Test if the process level feedback creates a more robust understanding of how to do the task in the framework of human reasoning and values.</p>

            <p>Check out this post by David Udell, <a href="https://www.lesswrong.com/posts/ZNXDRGshgoq3cmxhB/the-shard-theory-alignment-scheme">The Shard Theory Alignment Scheme,</a>
            for more info on our team's vision.</p>

            <!---->
            <!---->
            <!-- Header -->
            <header id="header">
                <div class="inner">


                    <!-- Nav -->
                    <nav>
                        <ul>
                            <li><a href="#menu">Menu</a></li>
                        </ul>
                    </nav>

                </div>
            </header>



            <!-- Menu -->
            <nav id="menu">
						<h2>Reports</h2>
                        <ul>
                            <li><a href="index.html">Home</a></li>
                            <li><a href="Orca.html">Orca IoT Marketing Demo</a></li>
                            <li><a href="GTRIScalingStudy.html">ML Scaling Study and Mixup GTRI</a></li>
                            <li><a href="MLClassProject.html">Semantic Segmentation ML Class Project</a></li>
                            <li><a href="Unisim.html">Simulation of Trading on Uniswap Class Project</a></li>
                            <li><a href="Composer.html">Blending Songs using an Autoencoder</a></li>
                            <li><a href="Carnegie.html">Carnegie Mellon Game Design in Unity 3D</a></li>
                            <li><a href="Shard.html">AI Alignment Team Shard</a></li>
                            <li><a href="FedRL.html">Benchmarking Federated RL</a></li>
                            <li><a href="resume.html">Resume</a></li>

                        </ul>
					</nav>



        </div>

        <!-- Scripts -->
        <script src="assets/js/jquery.min.js"></script>
        <script src="assets/js/browser.min.js"></script>
        <script src="assets/js/breakpoints.min.js"></script>
        <script src="assets/js/util.js"></script>
        <script src="assets/js/main.js"></script>

    </body>
</html>